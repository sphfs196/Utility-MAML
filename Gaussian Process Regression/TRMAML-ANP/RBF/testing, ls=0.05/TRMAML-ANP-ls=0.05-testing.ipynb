{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import time\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The (A)NP takes as input a `NPRegressionDescription` namedtuple with fields:\n",
    "#   `query`: a tuple containing ((context_x, context_y), target_x)\n",
    "#   `target_y`: a tensor containing the ground truth for the targets to be\n",
    "#     predicted\n",
    "#   `num_total_points`: A vector containing a scalar that describes the total\n",
    "#     number of datapoints used (context + target)\n",
    "#   `num_context_points`: A vector containing a scalar that describes the number\n",
    "#     of datapoints used as context\n",
    "# The GPCurvesReader returns the newly sampled data in this format at each\n",
    "# iteration\n",
    "\n",
    "NPRegressionDescription = collections.namedtuple(\n",
    "    \"NPRegressionDescription\",\n",
    "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
    "\n",
    "\n",
    "class GPCurvesReader(object):\n",
    "  \"\"\"Generates curves using a Gaussian Process (GP).\n",
    "\n",
    "  Supports vector inputs (x) and vector outputs (y). Kernel is\n",
    "  mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
    "  some factor chosen randomly in a range. Outputs are independent gaussian\n",
    "  processes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               batch_size,\n",
    "               max_num_context,\n",
    "               x_size=1,\n",
    "               y_size=1,\n",
    "               l1_scale=0.05,\n",
    "               sigma_scale=1.0,\n",
    "               random_kernel_parameters=False,\n",
    "               testing=False):\n",
    "    \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
    "\n",
    "    Args:\n",
    "      batch_size: An integer.\n",
    "      max_num_context: The max number of observations in the context.\n",
    "      x_size: Integer >= 1 for length of \"x values\" vector.\n",
    "      y_size: Integer >= 1 for length of \"y values\" vector.\n",
    "      l1_scale: Float; typical scale for kernel distance function.\n",
    "      sigma_scale: Float; typical scale for variance.\n",
    "      random_kernel_parameters: If `True`, the kernel parameters (l1 and sigma) \n",
    "          will be sampled uniformly within [0.1, l1_scale] and [0.1, sigma_scale].\n",
    "      testing: Boolean that indicates whether we are testing. If so there are\n",
    "          more targets for visualization.\n",
    "    \"\"\"\n",
    "    self._batch_size = batch_size\n",
    "    self._max_num_context = max_num_context\n",
    "    self._x_size = x_size\n",
    "    self._y_size = y_size\n",
    "    self._l1_scale = l1_scale\n",
    "    self._sigma_scale = sigma_scale\n",
    "    self._random_kernel_parameters = random_kernel_parameters\n",
    "    self._testing = testing\n",
    "\n",
    "  def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
    "    \"\"\"Applies the Gaussian kernel to generate curve data.\n",
    "\n",
    "    Args:\n",
    "      xdata: Tensor of shape [B, num_total_points, x_size] with\n",
    "          the values of the x-axis data.\n",
    "      l1: Tensor of shape [B, y_size, x_size], the scale\n",
    "          parameter of the Gaussian kernel.\n",
    "      sigma_f: Tensor of shape [B, y_size], the magnitude\n",
    "          of the std.\n",
    "      sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "    Returns:\n",
    "      The kernel, a float tensor of shape\n",
    "      [B, y_size, num_total_points, num_total_points].\n",
    "    \"\"\"\n",
    "    num_total_points = tf.shape(xdata)[1]\n",
    "\n",
    "    # Expand and take the difference\n",
    "    xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n",
    "    xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n",
    "    diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points, x_size]\n",
    "    norm = tf.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
    "\n",
    "    norm = tf.reduce_sum(\n",
    "        norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
    "\n",
    "    # [B, y_size, num_total_points, num_total_points]\n",
    "    kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-0.5 * norm)\n",
    "\n",
    "    # Add some noise to the diagonal to make the cholesky work.\n",
    "    kernel += (sigma_noise**2) * tf.eye(num_total_points)\n",
    "\n",
    "    return kernel\n",
    "\n",
    "  def generate_curves(self):\n",
    "    \"\"\"Builds the op delivering the data.\n",
    "\n",
    "    Generated functions are `float32` with x values between -2 and 2.\n",
    "    \n",
    "    Returns:\n",
    "      A `NPRegressionDescription` namedtuple.\n",
    "    \"\"\"\n",
    "    num_context = tf.random_uniform(\n",
    "        shape=[], minval=3, maxval=self._max_num_context, dtype=tf.int32)\n",
    "\n",
    "    # If we are testing we want to have more targets and have them evenly\n",
    "    # distributed in order to plot the function.\n",
    "    if self._testing:\n",
    "      num_target = 400\n",
    "      num_total_points = num_target\n",
    "      x_values = tf.tile(\n",
    "          tf.expand_dims(tf.range(-2., 2., 1. / 100, dtype=tf.float32), axis=0),\n",
    "          [self._batch_size, 1])\n",
    "      x_values = tf.expand_dims(x_values, axis=-1)\n",
    "    # During training the number of target points and their x-positions are\n",
    "    # selected at random\n",
    "    else:\n",
    "      num_target = tf.random_uniform(shape=(), minval=0, \n",
    "                                     maxval=self._max_num_context - num_context,\n",
    "                                     dtype=tf.int32)\n",
    "      num_total_points = num_context + num_target\n",
    "      x_values = tf.random_uniform(\n",
    "          [self._batch_size, num_total_points, self._x_size], -2, 2)\n",
    "\n",
    "    # Set kernel parameters\n",
    "    # Either choose a set of random parameters for the mini-batch\n",
    "    if self._random_kernel_parameters:\n",
    "      l1 = tf.random_uniform([self._batch_size, self._y_size,\n",
    "                              self._x_size], 0.1, self._l1_scale)\n",
    "      sigma_f = tf.random_uniform([self._batch_size, self._y_size],\n",
    "                                  0.1, self._sigma_scale)\n",
    "    # Or use the same fixed parameters for all mini-batches\n",
    "    else:\n",
    "      l1 = tf.ones(shape=[self._batch_size, self._y_size,\n",
    "                          self._x_size]) * self._l1_scale\n",
    "      sigma_f = tf.ones(shape=[self._batch_size,\n",
    "                               self._y_size]) * self._sigma_scale\n",
    "\n",
    "    # Pass the x_values through the Gaussian kernel\n",
    "    # [batch_size, y_size, num_total_points, num_total_points]\n",
    "    kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "\n",
    "    # Calculate Cholesky, using double precision for better stability:\n",
    "    cholesky = tf.cast(tf.cholesky(tf.cast(kernel, tf.float64)), tf.float32)\n",
    "\n",
    "    # Sample a curve\n",
    "    # [batch_size, y_size, num_total_points, 1]\n",
    "    y_values = tf.matmul(\n",
    "        cholesky,\n",
    "        tf.random_normal([self._batch_size, self._y_size, num_total_points, 1]))\n",
    "\n",
    "    # [batch_size, num_total_points, y_size]\n",
    "    y_values = tf.transpose(tf.squeeze(y_values, 3), [0, 2, 1])\n",
    "\n",
    "    if self._testing:\n",
    "      # Select the targets\n",
    "      target_x = x_values\n",
    "      target_y = y_values\n",
    "\n",
    "      # Select the observations\n",
    "      idx = tf.random_shuffle(tf.range(num_target))\n",
    "      context_x = tf.gather(x_values, idx[:num_context], axis=1)\n",
    "      context_y = tf.gather(y_values, idx[:num_context], axis=1)\n",
    "\n",
    "    else:\n",
    "      # Select the targets which will consist of the context points as well as\n",
    "      # some new target points\n",
    "      target_x = x_values[:, :num_target + num_context, :]\n",
    "      target_y = y_values[:, :num_target + num_context, :]\n",
    "\n",
    "      # Select the observations\n",
    "      context_x = x_values[:, :num_context, :]\n",
    "      context_y = y_values[:, :num_context, :]\n",
    "\n",
    "    query = ((context_x, context_y), target_x)\n",
    "\n",
    "    return NPRegressionDescription(\n",
    "        query=query,\n",
    "        target_y=target_y,\n",
    "        num_total_points=tf.shape(target_x)[1],\n",
    "        num_context_points=num_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility methods\n",
    "def batch_mlp(input, output_sizes, variable_scope):\n",
    "  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n",
    "  \n",
    "  Args:\n",
    "    input: input tensor of shape [B,n,d_in].\n",
    "    output_sizes: An iterable containing the output sizes of the MLP as defined \n",
    "        in `basic.Linear`.\n",
    "    variable_scope: String giving the name of the variable scope. If this is set\n",
    "        to be the same as a previously defined MLP, then the weights are reused.\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B,n,d_out] where d_out=output_sizes[-1]\n",
    "  \"\"\"\n",
    "  # Get the shapes of the input and reshape to parallelise across observations\n",
    "  batch_size, _, filter_size = input.shape.as_list()\n",
    "#   batch_size = tf.shape(input)[0]\n",
    "#   filter_size = tf.shape(input)[2]\n",
    "  output = tf.reshape(input, (-1, filter_size))\n",
    "  output.set_shape((None, filter_size))\n",
    "\n",
    "  # Pass through MLP\n",
    "  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
    "    for i, size in enumerate(output_sizes[:-1]):\n",
    "      output = tf.nn.relu(\n",
    "          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n",
    "\n",
    "    # Last layer without a ReLu\n",
    "    output = tf.layers.dense(\n",
    "        output, output_sizes[-1], name=\"layer_{}\".format(i + 1))\n",
    "\n",
    "  # Bring back into original shape\n",
    "  output = tf.reshape(output, (batch_size, -1, output_sizes[-1]))\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicEncoder(object):\n",
    "  \"\"\"The Deterministic Encoder.\"\"\"\n",
    "\n",
    "  def __init__(self, output_sizes, attention):\n",
    "    \"\"\"(A)NP deterministic encoder.\n",
    "\n",
    "    Args:\n",
    "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
    "      attention: The attention module.\n",
    "    \"\"\"\n",
    "    self._output_sizes = output_sizes\n",
    "    self._attention = attention\n",
    "\n",
    "  def __call__(self, context_x, context_y, target_x):\n",
    "    \"\"\"Encodes the inputs into one representation.\n",
    "\n",
    "    Args:\n",
    "      context_x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
    "          task this corresponds to the x-values.\n",
    "      context_y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
    "          task this corresponds to the y-values.\n",
    "      target_x: Tensor of shape [B,target_observations,d_x]. \n",
    "          For this 1D regression task this corresponds to the x-values.\n",
    "\n",
    "    Returns:\n",
    "      The encoded representation. Tensor of shape [B,target_observations,d]\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate x and y along the filter axes\n",
    "    encoder_input = tf.concat([context_x, context_y], axis=-1)\n",
    "\n",
    "    # Pass final axis through MLP\n",
    "    hidden = batch_mlp(encoder_input, self._output_sizes, \n",
    "                       \"deterministic_encoder\")\n",
    "\n",
    "    # Apply attention\n",
    "    with tf.variable_scope(\"deterministic_encoder\", reuse=tf.AUTO_REUSE):\n",
    "        hidden = self._attention(context_x, target_x, hidden)\n",
    "\n",
    "    return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentEncoder(object):\n",
    "  \"\"\"The Latent Encoder.\"\"\"\n",
    "\n",
    "  def __init__(self, output_sizes, num_latents):\n",
    "    \"\"\"(A)NP latent encoder.\n",
    "\n",
    "    Args:\n",
    "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
    "      num_latents: The latent dimensionality.\n",
    "    \"\"\"\n",
    "    self._output_sizes = output_sizes\n",
    "    self._num_latents = num_latents\n",
    "\n",
    "  def __call__(self, x, y):\n",
    "    \"\"\"Encodes the inputs into one representation.\n",
    "\n",
    "    Args:\n",
    "      x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
    "          task this corresponds to the x-values.\n",
    "      y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
    "          task this corresponds to the y-values.\n",
    "\n",
    "    Returns:\n",
    "      A normal distribution over tensors of shape [B, num_latents]\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate x and y along the filter axes\n",
    "    encoder_input = tf.concat([x, y], axis=-1)\n",
    "\n",
    "    # Pass final axis through MLP\n",
    "    hidden = batch_mlp(encoder_input, self._output_sizes, \"latent_encoder\")\n",
    "      \n",
    "    # Aggregator: take the mean over all points\n",
    "    hidden = tf.reduce_mean(hidden, axis=1)\n",
    "    \n",
    "    # Have further MLP layers that map to the parameters of the Gaussian latent\n",
    "    with tf.variable_scope(\"latent_encoder\", reuse=tf.AUTO_REUSE):\n",
    "      # First apply intermediate relu layer \n",
    "      hidden = tf.nn.relu(\n",
    "          tf.layers.dense(hidden, \n",
    "                          (self._output_sizes[-1] + self._num_latents)/2, \n",
    "                          name=\"penultimate_layer\"))\n",
    "      # Then apply further linear layers to output latent mu and log sigma\n",
    "      mu = tf.layers.dense(hidden, self._num_latents, name=\"mean_layer\")\n",
    "      log_sigma = tf.layers.dense(hidden, self._num_latents, name=\"std_layer\")\n",
    "      \n",
    "    # Compute sigma\n",
    "    sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n",
    "\n",
    "    return tf.contrib.distributions.Normal(loc=mu, scale=sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "  \"\"\"The Decoder.\"\"\"\n",
    "\n",
    "  def __init__(self, output_sizes):\n",
    "    \"\"\"(A)NP decoder.\n",
    "\n",
    "    Args:\n",
    "      output_sizes: An iterable containing the output sizes of the decoder MLP \n",
    "          as defined in `basic.Linear`.\n",
    "    \"\"\"\n",
    "    self._output_sizes = output_sizes\n",
    "\n",
    "  def __call__(self, representation, target_x):\n",
    "    \"\"\"Decodes the individual targets.\n",
    "\n",
    "    Args:\n",
    "      representation: The representation of the context for target predictions. \n",
    "          Tensor of shape [B,target_observations,?].\n",
    "      target_x: The x locations for the target query.\n",
    "          Tensor of shape [B,target_observations,d_x].\n",
    "\n",
    "    Returns:\n",
    "      dist: A multivariate Gaussian over the target points. A distribution over\n",
    "          tensors of shape [B,target_observations,d_y].\n",
    "      mu: The mean of the multivariate Gaussian.\n",
    "          Tensor of shape [B,target_observations,d_x].\n",
    "      sigma: The standard deviation of the multivariate Gaussian.\n",
    "          Tensor of shape [B,target_observations,d_x].\n",
    "    \"\"\"\n",
    "    # concatenate target_x and representation\n",
    "    hidden = tf.concat([representation, target_x], axis=-1)\n",
    "    \n",
    "    # Pass final axis through MLP\n",
    "    hidden = batch_mlp(hidden, self._output_sizes, \"decoder\")\n",
    "\n",
    "    # Get the mean an the variance\n",
    "    mu, log_sigma = tf.split(hidden, 2, axis=-1)\n",
    "\n",
    "    # Bound the variance\n",
    "    sigma = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n",
    "\n",
    "    # Get the distribution\n",
    "    dist = tf.contrib.distributions.MultivariateNormalDiag(\n",
    "        loc=mu, scale_diag=sigma)\n",
    "\n",
    "    return dist, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentModel(object):\n",
    "    \"\"\"The (A)NP model.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_encoder_output_sizes, num_latents,\n",
    "               decoder_output_sizes, use_deterministic_path=True, \n",
    "               deterministic_encoder_output_sizes=None, attention=None):\n",
    "        \"\"\"Initialises the model.\n",
    "\n",
    "        Args:\n",
    "          latent_encoder_output_sizes: An iterable containing the sizes of hidden \n",
    "              layers of the latent encoder.\n",
    "          num_latents: The latent dimensionality.\n",
    "          decoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
    "              the decoder. The last element should correspond to d_y * 2\n",
    "              (it encodes both mean and variance concatenated)\n",
    "          use_deterministic_path: a boolean that indicates whether the deterministic\n",
    "              encoder is used or not.\n",
    "          deterministic_encoder_output_sizes: An iterable containing the sizes of \n",
    "              hidden layers of the deterministic encoder. The last one is the size \n",
    "              of the deterministic representation r.\n",
    "          attention: The attention module used in the deterministic encoder.\n",
    "              Only relevant when use_deterministic_path=True.\n",
    "        \"\"\"\n",
    "        self._latent_encoder = LatentEncoder(latent_encoder_output_sizes, \n",
    "                                             num_latents)\n",
    "        self._decoder = Decoder(decoder_output_sizes)\n",
    "        self._use_deterministic_path = use_deterministic_path\n",
    "        if use_deterministic_path:\n",
    "            self._deterministic_encoder = DeterministicEncoder(\n",
    "              deterministic_encoder_output_sizes, attention)\n",
    "\n",
    "\n",
    "    def __call__(self, query, num_targets, target_y=None):\n",
    "        \"\"\"Returns the predicted mean and variance at the target points.\n",
    "\n",
    "        Args:\n",
    "          query: Array containing ((context_x, context_y), target_x) where:\n",
    "              context_x: Tensor of shape [B,num_contexts,d_x]. \n",
    "                  Contains the x values of the context points.\n",
    "              context_y: Tensor of shape [B,num_contexts,d_y]. \n",
    "                  Contains the y values of the context points.\n",
    "              target_x: Tensor of shape [B,num_targets,d_x]. \n",
    "                  Contains the x values of the target points.\n",
    "          num_targets: Number of target points.\n",
    "          target_y: The ground truth y values of the target y. \n",
    "              Tensor of shape [B,num_targets,d_y].\n",
    "\n",
    "        Returns:\n",
    "          log_p: The log_probability of the target_y given the predicted\n",
    "              distribution. Tensor of shape [B,num_targets].\n",
    "          mu: The mean of the predicted distribution. \n",
    "              Tensor of shape [B,num_targets,d_y].\n",
    "          sigma: The variance of the predicted distribution.\n",
    "              Tensor of shape [B,num_targets,d_y].\n",
    "        \"\"\"\n",
    "\n",
    "        (context_x, context_y), target_x = query\n",
    "\n",
    "        # Pass query through the encoder and the decoder\n",
    "        prior = self._latent_encoder(context_x, context_y)\n",
    "\n",
    "        # For training, when target_y is available, use targets for latent encoder.\n",
    "        # Note that targets contain contexts by design.\n",
    "        if target_y is None:\n",
    "            latent_rep = prior.sample()\n",
    "        # For testing, when target_y unavailable, use contexts for latent encoder.\n",
    "        else:\n",
    "            posterior = self._latent_encoder(target_x, target_y)\n",
    "            latent_rep = posterior.sample()\n",
    "        latent_rep = tf.tile(tf.expand_dims(latent_rep, axis=1),\n",
    "                             [1, num_targets, 1])\n",
    "        if self._use_deterministic_path:\n",
    "            deterministic_rep = self._deterministic_encoder(context_x, context_y,\n",
    "                                                          target_x)\n",
    "            representation = tf.concat([deterministic_rep, latent_rep], axis=-1)\n",
    "        else:\n",
    "            representation = latent_rep\n",
    "\n",
    "        dist, mu, sigma = self._decoder(representation, target_x)\n",
    "\n",
    "        # If we want to calculate the log_prob for training we will make use of the\n",
    "        # target_y. At test time the target_y is not available so we return None.\n",
    "        if target_y is not None:\n",
    "            log_p = dist.log_prob(target_y)\n",
    "            posterior = self._latent_encoder(target_x, target_y)\n",
    "            kl = tf.reduce_sum(\n",
    "              tf.contrib.distributions.kl_divergence(posterior, prior), \n",
    "              axis=-1, keepdims=True)\n",
    "            kl = tf.tile(kl, [1, num_targets])\n",
    "            loss = - tf.reduce_mean(log_p - kl / tf.cast(num_targets, tf.float32))\n",
    "            \n",
    "        else:\n",
    "            log_p = None\n",
    "            kl = None\n",
    "            loss = None\n",
    "\n",
    "        return mu, sigma, log_p, kl, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_attention(q, v):\n",
    "  \"\"\"Uniform attention. Equivalent to np.\n",
    "\n",
    "  Args:\n",
    "    q: queries. tensor of shape [B,m,d_k].\n",
    "    v: values. tensor of shape [B,n,d_v].\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B,m,d_v].\n",
    "  \"\"\"\n",
    "  total_points = tf.shape(q)[1]\n",
    "  rep = tf.reduce_mean(v, axis=1, keepdims=True)  # [B,1,d_v]\n",
    "  rep = tf.tile(rep, [1, total_points, 1])\n",
    "  return rep\n",
    "\n",
    "def laplace_attention(q, k, v, scale, normalise):\n",
    "  \"\"\"Computes laplace exponential attention.\n",
    "\n",
    "  Args:\n",
    "    q: queries. tensor of shape [B,m,d_k].\n",
    "    k: keys. tensor of shape [B,n,d_k].\n",
    "    v: values. tensor of shape [B,n,d_v].\n",
    "    scale: float that scales the L1 distance.\n",
    "    normalise: Boolean that determines whether weights sum to 1.\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B,m,d_v].\n",
    "  \"\"\"\n",
    "  k = tf.expand_dims(k, axis=1)  # [B,1,n,d_k]\n",
    "  q = tf.expand_dims(q, axis=2)  # [B,m,1,d_k]\n",
    "  unnorm_weights = - tf.abs((k - q) / scale)  # [B,m,n,d_k]\n",
    "  unnorm_weights = tf.reduce_sum(unnorm_weights, axis=-1)  # [B,m,n]\n",
    "  if normalise:\n",
    "    weight_fn = tf.nn.softmax\n",
    "  else:\n",
    "    weight_fn = lambda x: 1 + tf.tanh(x)\n",
    "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
    "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
    "  return rep\n",
    "\n",
    "\n",
    "def dot_product_attention(q, k, v, normalise):\n",
    "  \"\"\"Computes dot product attention.\n",
    "\n",
    "  Args:\n",
    "    q: queries. tensor of  shape [B,m,d_k].\n",
    "    k: keys. tensor of shape [B,n,d_k].\n",
    "    v: values. tensor of shape [B,n,d_v].\n",
    "    normalise: Boolean that determines whether weights sum to 1.\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B,m,d_v].\n",
    "  \"\"\"\n",
    "  d_k = tf.shape(q)[-1]\n",
    "  scale = tf.sqrt(tf.cast(d_k, tf.float32))\n",
    "  unnorm_weights = tf.einsum('bjk,bik->bij', k, q) / scale  # [B,m,n]\n",
    "  if normalise:\n",
    "    weight_fn = tf.nn.softmax\n",
    "  else:\n",
    "    weight_fn = tf.sigmoid\n",
    "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
    "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
    "  return rep\n",
    "\n",
    "\n",
    "def multihead_attention(q, k, v, num_heads=8):\n",
    "  \"\"\"Computes multi-head attention.\n",
    "\n",
    "  Args:\n",
    "    q: queries. tensor of  shape [B,m,d_k].\n",
    "    k: keys. tensor of shape [B,n,d_k].\n",
    "    v: values. tensor of shape [B,n,d_v].\n",
    "    num_heads: number of heads. Should divide d_v.\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B,m,d_v].\n",
    "  \"\"\"\n",
    "  d_k = q.get_shape().as_list()[-1]\n",
    "  d_v = v.get_shape().as_list()[-1]\n",
    "  head_size = d_v / num_heads\n",
    "  key_initializer = tf.random_normal_initializer(stddev=d_k**-0.5)\n",
    "  value_initializer = tf.random_normal_initializer(stddev=d_v**-0.5)\n",
    "  rep = tf.constant(0.0)\n",
    "  for h in range(num_heads):\n",
    "    o = dot_product_attention(\n",
    "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
    "                   name='wq%d' % h, use_bias=False, padding='VALID')(q),\n",
    "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
    "                   name='wk%d' % h, use_bias=False, padding='VALID')(k),\n",
    "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
    "                   name='wv%d' % h, use_bias=False, padding='VALID')(v),\n",
    "        normalise=True)\n",
    "    rep += tf.layers.Conv1D(d_v, 1, kernel_initializer=value_initializer,\n",
    "                      name='wo%d' % h, use_bias=False, padding='VALID')(o)\n",
    "  return rep\n",
    "\n",
    "class Attention(object):\n",
    "  \"\"\"The Attention module.\"\"\"\n",
    "\n",
    "  def __init__(self, rep, output_sizes, att_type, scale=1., normalise=True,\n",
    "               num_heads=8):\n",
    "    \"\"\"Create attention module.\n",
    "\n",
    "    Takes in context inputs, target inputs and\n",
    "    representations of each context input/output pair\n",
    "    to output an aggregated representation of the context data.\n",
    "    Args:\n",
    "      rep: transformation to apply to contexts before computing attention. \n",
    "          One of: ['identity','mlp'].\n",
    "      output_sizes: list of number of hidden units per layer of mlp.\n",
    "          Used only if rep == 'mlp'.\n",
    "      att_type: type of attention. One of the following:\n",
    "          ['uniform','laplace','dot_product','multihead']\n",
    "      scale: scale of attention.\n",
    "      normalise: Boolean determining whether to:\n",
    "          1. apply softmax to weights so that they sum to 1 across context pts or\n",
    "          2. apply custom transformation to have weights in [0,1].\n",
    "      num_heads: number of heads for multihead.\n",
    "    \"\"\"\n",
    "    self._rep = rep\n",
    "    self._output_sizes = output_sizes\n",
    "    self._type = att_type\n",
    "    self._scale = scale\n",
    "    self._normalise = normalise\n",
    "    if self._type == 'multihead':\n",
    "      self._num_heads = num_heads\n",
    "\n",
    "  def __call__(self, x1, x2, r):\n",
    "    \"\"\"Apply attention to create aggregated representation of r.\n",
    "\n",
    "    Args:\n",
    "      x1: tensor of shape [B,n1,d_x].\n",
    "      x2: tensor of shape [B,n2,d_x].\n",
    "      r: tensor of shape [B,n1,d].\n",
    "      \n",
    "    Returns:\n",
    "      tensor of shape [B,n2,d]\n",
    "\n",
    "    Raises:\n",
    "      NameError: The argument for rep/type was invalid.\n",
    "    \"\"\"\n",
    "    if self._rep == 'identity':\n",
    "      k, q = (x1, x2)\n",
    "    elif self._rep == 'mlp':\n",
    "      # Pass through MLP\n",
    "      k = batch_mlp(x1, self._output_sizes, \"attention\")\n",
    "      q = batch_mlp(x2, self._output_sizes, \"attention\")\n",
    "    else:\n",
    "      raise NameError(\"'rep' not among ['identity','mlp']\")\n",
    "\n",
    "    if self._type == 'uniform':\n",
    "      rep = uniform_attention(q, r)\n",
    "    elif self._type == 'laplace':\n",
    "      rep = laplace_attention(q, k, r, self._scale, self._normalise)\n",
    "    elif self._type == 'dot_product':\n",
    "      rep = dot_product_attention(q, k, r, self._normalise)\n",
    "    elif self._type == 'multihead':\n",
    "      rep = multihead_attention(q, k, r, self._num_heads)\n",
    "    else:\n",
    "      raise NameError((\"'att_type' not among ['uniform','laplace','dot_product'\"\n",
    "                       \",'multihead']\"))\n",
    "\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_functions(target_x, target_y, context_x, context_y, pred_y, std):\n",
    "  \"\"\"Plots the predicted mean and variance and the context points.\n",
    "  \n",
    "  Args: \n",
    "    target_x: An array of shape [B,num_targets,1] that contains the\n",
    "        x values of the target points.\n",
    "    target_y: An array of shape [B,num_targets,1] that contains the\n",
    "        y values of the target points.\n",
    "    context_x: An array of shape [B,num_contexts,1] that contains \n",
    "        the x values of the context points.\n",
    "    context_y: An array of shape [B,num_contexts,1] that contains \n",
    "        the y values of the context points.\n",
    "    pred_y: An array of shape [B,num_targets,1] that contains the\n",
    "        predicted means of the y values at the target points in target_x.\n",
    "    std: An array of shape [B,num_targets,1] that contains the\n",
    "        predicted std dev of the y values at the target points in target_x.\n",
    "  \"\"\"\n",
    "  # Plot everything\n",
    "  plt.plot(target_x[0], pred_y[0], 'b', linewidth=2)\n",
    "  plt.plot(target_x[0], target_y[0], 'k:', linewidth=2)\n",
    "  plt.plot(context_x[0], context_y[0], 'ko', markersize=10)\n",
    "  plt.fill_between(\n",
    "      target_x[0, :, 0],\n",
    "      pred_y[0, :, 0] - std[0, :, 0],\n",
    "      pred_y[0, :, 0] + std[0, :, 0],\n",
    "      alpha=0.2,\n",
    "      facecolor='#65c9f7',\n",
    "      interpolate=True)\n",
    "\n",
    "  # Make the plot pretty\n",
    "  plt.yticks([-2, 0, 2], fontsize=16)\n",
    "  plt.xticks([-2, 0, 2], fontsize=16)\n",
    "  plt.ylim([-2, 2])\n",
    "  plt.grid('off')\n",
    "  ax = plt.gca()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = 100000 #@param {type:\"number\"}\n",
    "MAX_CONTEXT_POINTS = 100 #@param {type:\"number\"}\n",
    "PLOT_AFTER = 1000\n",
    "HIDDEN_SIZE = 128 #@param {type:\"number\"}\n",
    "MODEL_TYPE = 'ANP' #@param ['NP','ANP']\n",
    "ATTENTION_TYPE = 'multihead' #@param ['uniform','laplace','dot_product','multihead']\n",
    "random_kernel_parameters = False #@param {type:\"boolean\"}\n",
    "\n",
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "dataset_test = GPCurvesReader(\n",
    "    batch_size=10, max_num_context=MAX_CONTEXT_POINTS, l1_scale=0.05, random_kernel_parameters=random_kernel_parameters, testing=True)\n",
    "data_test = dataset_test.generate_curves()\n",
    "\n",
    "\n",
    "# Sizes of the layers of the MLPs for the encoders and decoder\n",
    "# The final output layer of the decoder outputs two values, one for the mean and\n",
    "# one for the variance of the prediction at the target location\n",
    "latent_encoder_output_sizes = [HIDDEN_SIZE]*4\n",
    "num_latents = HIDDEN_SIZE\n",
    "deterministic_encoder_output_sizes= [HIDDEN_SIZE]*4\n",
    "decoder_output_sizes = [HIDDEN_SIZE]*2 + [2]\n",
    "use_deterministic_path = True\n",
    "\n",
    "# ANP with multihead attention\n",
    "if MODEL_TYPE == 'ANP':\n",
    "  attention = Attention(rep='mlp', output_sizes=[HIDDEN_SIZE]*2, \n",
    "                        att_type='multihead')\n",
    "# NP - equivalent to uniform attention\n",
    "elif MODEL_TYPE == 'NP':\n",
    "  attention = Attention(rep='identity', output_sizes=None, att_type='uniform')\n",
    "else:\n",
    "  raise NameError(\"MODEL_TYPE not among ['ANP,'NP']\")\n",
    "\n",
    "# Define the model\n",
    "model = LatentModel(latent_encoder_output_sizes, num_latents,\n",
    "                    decoder_output_sizes, use_deterministic_path, \n",
    "                    deterministic_encoder_output_sizes, attention)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "\n",
    "\n",
    "# test, Get the predicted mean and variance at the target points for the testing set\n",
    "mu, sigma, _, _, loss_test = model(data_test.query, data_test.num_total_points, data_test.target_y)\n",
    "test_train_step = optimizer.minimize(loss_test)\n",
    "\n",
    "# reset variables (theta)\n",
    "var_list = tf.trainable_variables()\n",
    "var_value = tf.placeholder(tf.float32)\n",
    "for var_idx, var in enumerate(var_list) : \n",
    "    globals()['reset_theta_' + str(var_idx)] = tf.assign(var, var_value)\n",
    "\n",
    "# input context points\n",
    "context_x_new = tf.placeholder(tf.float32, shape=[1, None, 1])\n",
    "context_y_new = tf.placeholder(tf.float32, shape=[1, None, 1])\n",
    "target_x_new = tf.placeholder(tf.float32, shape=[1, None, 1])\n",
    "new_query = ((context_x_new, context_y_new), target_x_new)\n",
    "\n",
    "# predict posterior\n",
    "mu_MAMLANP_new, sigma_MAMLANP_new, _, _, _ = model(new_query, 400) # 400 = target_num\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, '../save_anp_models/MAML_ANP_lr_(1e-3, 5e-4)_TRMAML-setting(ls_interval, obj no avg)_200k_batch_16_uniform_sample_ver')\n",
    "\n",
    "loss_test_value, pred_y, var, target_y, whole_query = sess.run(\n",
    "          [loss_test, mu, sigma, data_test.target_y, data_test.query])\n",
    "\n",
    "(context_x, context_y), target_x = whole_query\n",
    "\n",
    "# Plot the prediction and the context\n",
    "plot_functions(target_x, target_y, context_x, context_y, pred_y, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from GPy.models import GPRegression\n",
    "from emukit.model_wrappers.gpy_model_wrappers import GPyModelWrapper\n",
    "import GPy\n",
    "\n",
    "import time\n",
    "\n",
    "### --- Figure config\n",
    "LEGEND_SIZE = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load true functions\n",
    "trueF_list = list(np.load('./true_functions/trueF_list_from_NP_500.npy', allow_pickle=True))\n",
    "target_y_list = list(np.load('./true_functions/target_y_from_NP_500.npy', allow_pickle=True))\n",
    "target_x_list = list(np.load('./true_functions/target_x_from_NP_500.npy', allow_pickle=True))\n",
    "sample_idx_list = list(np.load('./50sample_idx.npy'))\n",
    "len(target_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_theta(theta) : \n",
    "    theta_new = tf.trainable_variables()\n",
    "    for var_idx, var in enumerate(theta_new) : \n",
    "        sess.run(globals()['reset_theta_' + str(var_idx)], feed_dict={var_value : theta[var_idx]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu_1, sigma_1, mu_2, sigma_2):\n",
    "    term_1 = np.log(sigma_2 / sigma_1)\n",
    "    term_2 = (np.power(sigma_1, 2) + np.power((mu_1 - mu_2), 2)) / (2 * np.power(sigma_2, 2))\n",
    "    return term_1 + term_2 - 0.5\n",
    "\n",
    "def JS_divergence(mu_1, sigma_1, mu_2, sigma_2):\n",
    "    mu_avg = 0.5 * (mu_1 + mu_2)\n",
    "    sigma_avg = np.sqrt(0.5 * ((np.power(sigma_1, 2) + np.power(sigma_2, 2))))\n",
    "    return 0.5 * kl_divergence(mu_1, sigma_1, mu_avg, sigma_avg) + 0.5 * kl_divergence(mu_2, sigma_2, mu_avg, sigma_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JS_list = []\n",
    "func_JSs = []\n",
    "target_num = 400\n",
    "target_x = target_x_list[0]\n",
    "\n",
    "trainable_vars = tf.trainable_variables()\n",
    "theta = [var.eval(session=sess) for var in trainable_vars]\n",
    "\n",
    "### iterate all the true functions\n",
    "for i in range(len(target_y_list)) : \n",
    "    JS_list = []\n",
    "    ### compute the posterior while giving a new sample point\n",
    "    for sample_idx in range(1, len(sample_idx_list)+1) : \n",
    "\n",
    "        ### fine tunning ###\n",
    "        reset_theta(theta) # 把上一次fine tunning過的參數調回來    \n",
    "        for k in range(3) : # 3 means fine tune 3 times. You can decide how many times you want to fine tune.\n",
    "            sess.run(test_train_step)\n",
    "\n",
    "        target_y = target_y_list[i]\n",
    "\n",
    "        ### You can rename these variable!!! ###\n",
    "        # target_x, target_y : true functions\n",
    "        # x_RBF, y_RBF : reshaped target_x, target_y\n",
    "        x_RBF = np.squeeze(target_x, axis=0)\n",
    "        y_RBF = np.squeeze(target_y, axis=0)\n",
    "\n",
    "        # x_init_RBF, y_init_RBF : sample points from target_x, target_y\n",
    "        x_init_RBF = np.array([x_RBF[index] for index in sample_idx_list[0:sample_idx]])\n",
    "        y_init_RBF = np.array([y_RBF[index] for index in sample_idx_list[0:sample_idx]])\n",
    "\n",
    "        set_x_RBF = x_init_RBF\n",
    "        set_y_RBF = y_init_RBF\n",
    "\n",
    "        # context_x_tensor, context_y_tensor : sample points from target_x, target_y. Reshaped x_init_RBF, y_init_RBF. \n",
    "        context_x_tensor = np.expand_dims(set_x_RBF, axis=0)\n",
    "        context_y_tensor = np.expand_dims(set_y_RBF, axis=0)\n",
    "        target_x_tensor = target_x\n",
    "        feed_dict = {context_x_new : context_x_tensor, \n",
    "                     context_y_new : context_y_tensor, \n",
    "                     target_x_new : target_x_tensor\n",
    "                    }\n",
    "\n",
    "        # compute posterior by ANP\n",
    "        mu_MAMLANP, sigma_MAMLANP = sess.run([mu_MAMLANP_new, sigma_MAMLANP_new], feed_dict=feed_dict)\n",
    "        mu_MAMLANP = np.squeeze(mu_MAMLANP, axis=0)\n",
    "        sigma_MAMLANP = np.squeeze(sigma_MAMLANP, axis=0)\n",
    "\n",
    "        # compute posterior by GP\n",
    "        gpy_model_RBF = GPy.models.GPRegression(x_init_RBF, y_init_RBF, GPy.kern.RBF(1, lengthscale=0.05, variance=1), noise_var=2e-2**2)\n",
    "        gpy_model_RBF.Gaussian_noise.variance.fix()\n",
    "        emukit_model_RBF = GPyModelWrapper(gpy_model_RBF)\n",
    "    \n",
    "        mu_GP, var_GP = emukit_model_RBF.predict(x_RBF)\n",
    "        sigma_GP = np.sqrt(var_GP)\n",
    "        \n",
    "        # compute JSD between the posterior of ANP and the posterior of GP\n",
    "        JS_sum = 0\n",
    "        for k in range(len(x_RBF)) : \n",
    "            JS = JS_divergence(mu_GP[k], sigma_GP[k], mu_MAMLANP[k], sigma_MAMLANP[k])\n",
    "            JS_sum += JS\n",
    "        JS_list.append(float(JS_sum))\n",
    "        \n",
    "        # plot everything\n",
    "        if i % 50 == 0 and sample_idx % 10 == 0 : \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.plot(set_x_RBF, set_y_RBF, \"ro\", markersize=10, label=\"Observations\")\n",
    "            plt.plot(x_RBF, y_RBF, \"k\", label=\"Objective Function\")\n",
    "\n",
    "            plt.plot(x_RBF, mu_MAMLANP, \"C0\", label=\"Model\")\n",
    "            plt.fill_between(x_RBF[:, 0],\n",
    "                             mu_MAMLANP[:, 0] + sigma_MAMLANP[:, 0],\n",
    "                             mu_MAMLANP[:, 0] - sigma_MAMLANP[:, 0], color=\"C0\", alpha=0.6)\n",
    "\n",
    "            plt.fill_between(x_RBF[:, 0],\n",
    "                             mu_MAMLANP[:, 0] + 2 * sigma_MAMLANP[:, 0],\n",
    "                             mu_MAMLANP[:, 0] - 2 * sigma_MAMLANP[:, 0], color=\"C0\", alpha=0.4)\n",
    "\n",
    "            plt.fill_between(x_RBF[:, 0],\n",
    "                             mu_MAMLANP[:, 0] + 3 * sigma_MAMLANP[:, 0],\n",
    "                             mu_MAMLANP[:, 0] - 3 * sigma_MAMLANP[:, 0], color=\"C0\", alpha=0.2)\n",
    "            plt.legend(loc=2, prop={'size': LEGEND_SIZE})\n",
    "            plt.xlabel(r\"$x$\")\n",
    "            plt.ylabel(r\"$f(x)$\")\n",
    "            plt.grid(True)\n",
    "            plt.xlim(-2, 2)\n",
    "            plt.title('TRMAML (ls = interval, batch 16, data 200k), ls = 0.05, function ' + str(i) + ', sample ' + str(sample_idx))\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.plot(set_x_RBF, set_y_RBF, \"ro\", markersize=10, label=\"Observations\")\n",
    "            plt.plot(x_RBF, y_RBF, \"k\", label=\"Objective Function\")\n",
    "\n",
    "            plt.plot(x_RBF, mu_GP, \"C0\", label=\"Model\")\n",
    "            plt.fill_between(x_RBF[:, 0],\n",
    "                             mu_GP[:, 0] + sigma_GP[:, 0],\n",
    "                             mu_GP[:, 0] - sigma_GP[:, 0], color=\"C0\", alpha=0.6)\n",
    "\n",
    "            plt.fill_between(x_RBF[:, 0],\n",
    "                             mu_GP[:, 0] + 2 * sigma_GP[:, 0],\n",
    "                             mu_GP[:, 0] - 2 * sigma_GP[:, 0], color=\"C0\", alpha=0.4)\n",
    "\n",
    "            plt.fill_between(x_RBF[:, 0],\n",
    "                             mu_GP[:, 0] + 3 * sigma_GP[:, 0],\n",
    "                             mu_GP[:, 0] - 3 * sigma_GP[:, 0], color=\"C0\", alpha=0.2)\n",
    "            plt.legend(loc=2, prop={'size': LEGEND_SIZE})\n",
    "            plt.xlabel(r\"$x$\")\n",
    "            plt.ylabel(r\"$f(x)$\")\n",
    "            plt.grid(True)\n",
    "            plt.xlim(-2, 2)\n",
    "            plt.title('GP, function ' + str(i) + ', sample ' + str(sample_idx))\n",
    "            plt.show()\n",
    "    func_JSs.append(JS_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute the average JSD of all the true functions\n",
    "total_JS_each_sample = np.zeros(len(func_JSs[0]))\n",
    "for JS_score_list in func_JSs : \n",
    "    total_JS_each_sample = total_JS_each_sample + JS_score_list\n",
    "average_JS_each_sample = total_JS_each_sample / np.shape(func_JSs)[0]\n",
    "\n",
    "### plot the JS_sum in each sample iteration\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('JS score, 500 function average')\n",
    "plt.plot(average_JS_each_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./result_tmp/TRMAML_JSD_lr_1e-3_5e-4_ls_interval_05-075_r1dot5_uniform_sample_obj_no_avg_3update_data_200k_test_500_func_batch_16_v1', average_JS_each_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
